{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd092584-d351-421e-9de7-7a53eb37622a",
   "metadata": {},
   "source": [
    "# Using Vertex AI Vector Search and Vertex AI Embeddings for Text for StackOverflow Questions\n",
    "\n",
    "## Overview\n",
    "This lab demonstrates how to provide semantic search to a large dataset of questions from StackOverflow. Because of the large size of the datataset, you will query the questions using BigQuery. Then, you will create text embeddings from the questions using the Vertex AI Text-Embeddings API service and store them in a Cloud Storage bucket. Once all embeddings are created, you will create a Vertex AI Vector Search index for those embeddings, so that you can search them afterwards.\n",
    "\n",
    "The Vertex AI Text-Embeddings API enhances the process of generating text embeddings. These text embeddings, which are numerical representations of text, play a pivotal role in many tasks involving the identification of similar items, like Google searches, online shopping recommendations, and personalized music suggestions.\n",
    "\n",
    "Vertex AI Vector Search Engine service is a high-scale, low-latency solution, for finding similar vectors from a large corpus. Vector Search is a fully managed offering, further reducing operational overhead. It is built upon Approximate Nearest Neighbor (ANN) technology developed by Google Research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7989b589-07bf-48e9-8da0-a641b8f4a292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.70.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.14.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery[pandas] in ./.local/lib/python3.10/site-packages (3.26.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.21.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.35.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.9.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (15.0.2)\n",
      "Requirement already satisfied: db-dtypes<2.0.0dev,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from db-dtypes<2.0.0dev,>=0.3.0->google-cloud-bigquery[pandas]) (1.26.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.0->google-cloud-bigquery[pandas]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.0->google-cloud-bigquery[pandas]) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery[pandas]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "Installing collected packages: google-cloud-storage\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.14.0\n",
      "    Uninstalling google-cloud-storage-2.14.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\n",
      "Successfully installed google-cloud-storage-2.18.2\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c084bb-8260-4dcd-ae8c-f15df2aba570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d80ea5-d4b9-4864-91cb-f75f14d4ca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-west1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd0b408-a2c9-4f55-ae60-059e6d2b3b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project = PROJECT_ID,\n",
    "              location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0ac83-3307-44f7-8ff7-d01e985e7124",
   "metadata": {},
   "source": [
    "## Prepare the data in BigQuery\n",
    "The dataset used for this lab is the StackOverflow dataset. This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset.\n",
    "\n",
    "Stack Overflow is the largest online community for programmers to learn, share their knowledge, and advance their careers. Updated on a quarterly basis, this BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges. This dataset is updated to mirror the Stack Overflow content on the Internet Archive, and is also available through the Stack Exchange Data Explorer.\n",
    "\n",
    "The BigQuery table is too large to fit into memory, so you need to write a generator called query_bigquery_chunks to yield chunks of the dataframe for processing. Additionally, an extra column title_with_body is added, which is a concatenation of the question title and body that will be used for creating embeddings.\n",
    "\n",
    "Import the libraries and initialize the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4fc072-0878-4ea1-b0eb-e3bbb33b4049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Generator\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ec2d46-12e8-478e-b2f9-ec1f5f4bfb72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the BigQuery query for the remote dataset.\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.id, q.title, q.body\n",
    "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions` where Score>0 ORDER BY View_Count desc) AS q\n",
    "        LIMIT {limit} OFFSET {offset};\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38097a5-174c-47d7-a95b-3113ea120f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function to access the BigQuery data in chunks.\n",
    "\n",
    "def query_bigquery_chunks(\n",
    "    max_rows: int, rows_per_chunk: int, start_chunk: int = 0\n",
    ") -> Generator[pd.DataFrame, Any, None]:\n",
    "    for offset in range(start_chunk, max_rows, rows_per_chunk):\n",
    "        query = QUERY_TEMPLATE.format(limit=rows_per_chunk, offset=offset)\n",
    "        query_job = client.query(query)\n",
    "        rows = query_job.result()\n",
    "        df = rows.to_dataframe()\n",
    "        df[\"title_with_body\"] = df.title + \"\\n\" + df.body\n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e92c2e9-96ee-4c91-bdfb-f49e97d1266c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_with_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16238973</td>\n",
       "      <td>Why does the order of both a return and a thro...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;private static ext.clsPassageiro Co...</td>\n",
       "      <td>Why does the order of both a return and a thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16239147</td>\n",
       "      <td>iOS: Do not back up attribute?</td>\n",
       "      <td>&lt;p&gt;Our app has recently been rejected for viol...</td>\n",
       "      <td>iOS: Do not back up attribute?\\n&lt;p&gt;Our app has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16196338</td>\n",
       "      <td>JSON.stringify doesn't work with normal Javasc...</td>\n",
       "      <td>&lt;p&gt;I must be missing something here, but the f...</td>\n",
       "      <td>JSON.stringify doesn't work with normal Javasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8356058</td>\n",
       "      <td>OpenGL window isn't opening</td>\n",
       "      <td>&lt;p&gt;I have code from the OpenGLBook (openglbook...</td>\n",
       "      <td>OpenGL window isn't opening\\n&lt;p&gt;I have code fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7711137</td>\n",
       "      <td>how to remove old applet from browser's cache ...</td>\n",
       "      <td>&lt;p&gt;Is there a way to delete older version of a...</td>\n",
       "      <td>how to remove old applet from browser's cache ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  16238973  Why does the order of both a return and a thro...   \n",
       "1  16239147                     iOS: Do not back up attribute?   \n",
       "2  16196338  JSON.stringify doesn't work with normal Javasc...   \n",
       "3   8356058                        OpenGL window isn't opening   \n",
       "4   7711137  how to remove old applet from browser's cache ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <pre><code>private static ext.clsPassageiro Co...   \n",
       "1  <p>Our app has recently been rejected for viol...   \n",
       "2  <p>I must be missing something here, but the f...   \n",
       "3  <p>I have code from the OpenGLBook (openglbook...   \n",
       "4  <p>Is there a way to delete older version of a...   \n",
       "\n",
       "                                     title_with_body  \n",
       "0  Why does the order of both a return and a thro...  \n",
       "1  iOS: Do not back up attribute?\\n<p>Our app has...  \n",
       "2  JSON.stringify doesn't work with normal Javasc...  \n",
       "3  OpenGL window isn't opening\\n<p>I have code fr...  \n",
       "4  how to remove old applet from browser's cache ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a dataframe of 1000 rows for demonstration purposes.\n",
    "df = next(query_bigquery_chunks(max_rows=1000, rows_per_chunk=1000))\n",
    "\n",
    "# Examine the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a445b58-fc10-46c5-9e65-4d9295a4a570",
   "metadata": {},
   "source": [
    "## Create text embeddings from BigQuery data\n",
    "Load the Vertex AI Embeddings for Text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810f412e-02bc-4f3f-8949-01c43060b3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13ac718-926a-4e47-8998-9e7b592236d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an embedding method that uses the model.\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd489c5-7ca5-4a47-a452-8b4311184e3b",
   "metadata": {},
   "source": [
    "According to the documentation, each request can handle up to 5 text instances. So we will need to split the BigQuery question results in batches of 5 before sending to the embedding API.\n",
    "\n",
    "Create a generate_batches to split results in batches of 5 to be sent to the embeddings API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06b16f1-ad16-4451-a4db-4a234647441a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb14db-5e60-4d3b-b24d-599f2844e4af",
   "metadata": {},
   "source": [
    "Encapsulate the process of generating batches and calling the embeddings API in a method called encode_text_to_embedding_batched. This method also handles rate-limiting using time.sleep. For production use cases, you would want a more sophisticated rate-limiting mechanism that takes retries into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab0a5b0-675c-46bc-9991-3d17ee3bcb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7647da-9834-4cd8-8df4-b591ca9bb811",
   "metadata": {},
   "source": [
    "Test the encoding function by encoding a subset of data and see if the embeddings and distance metrics make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155c6a46-46d2-4f1e-80e6-53411d2deb93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6399723714846b8b397d6b53a36750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode a subset of questions for validation\n",
    "questions = df.title.tolist()[:500]\n",
    "is_successful, question_embeddings = encode_text_to_embedding_batched(\n",
    "    sentences=df.title.tolist()[:500]\n",
    ")\n",
    "\n",
    "# Filter for successfully embedded sentences\n",
    "questions = np.array(questions)[is_successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a35f4b-adc2-4556-a781-176457e3bc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# Save the dimension size for later usage when creating the Vertex AI Vector Search index.\n",
    "DIMENSIONS = len(question_embeddings[0])\n",
    "\n",
    "print(DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76f5e4-852b-40d9-863a-789d91c8b3c0",
   "metadata": {},
   "source": [
    "Sort questions in order of similarity. According to the embedding documentation, the similarity of embeddings is calculated using the dot-product, with np.dot. Once you have the similarity score, sort the results and print them for inspection. 1 means very similar, 0 means very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d7505da-4f91-4ae3-9f69-d9f916b80b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query question = How to define DTD without strict element order?\n",
      "\t0: How to define DTD without strict element order?: 0.9999987167880122\n",
      "\t1: How to merge multiple items from an Xml field in SQL Server: 0.45985864810829613\n",
      "\t2: \"The 'http://www.w3.org/XML/1998/namespace:lang' attribute is not declared.\": 0.43271876314979474\n",
      "\t3: Lxml cssselect wildcard: 0.4297562877172967\n",
      "\t4: XSL changing spaces in a link to %20: 0.425946026449363\n",
      "\t5: Parameterized method with Ordering?: 0.424112698947372\n",
      "\t6: Regex for matching a previous group in the pattern?: 0.41796161033082024\n",
      "\t7: <span> vs <figure> vs <area>: 0.4143188733723252\n",
      "\t8: How to \"partially\" fulfill a sales order?: 0.41260790804391795\n",
      "\t9: Serialize with java 7, deserialize with java 6?: 0.4074802316452574\n",
      "\t10: HTML5 vs HTML4 - h1 tag rendered with extra space - how to remove?: 0.3834107020111859\n",
      "\t11: Any algorithm/approach to 'simulate' a cyclic graph over time?: 0.3829617080651104\n",
      "\t12: Is there a way to use ctrl-d as forward delete in Scala's REPL?: 0.3791163468365868\n",
      "\t13: Why does the order of both a return and a throws statement cause different warnings about unreachable code: 0.36964476781941563\n",
      "\t14: Dynamic ListView headers: 0.36273982437670504\n",
      "\t15: How to test concurrent access to resource (cache) in Perl?: 0.36244824263322556\n",
      "\t16: Using a composite key in GORM: 0.3619722964446557\n",
      "\t17: how to remove old applet from browser's cache programmatically?: 0.35674264760740493\n",
      "\t18: How to add dynamic size view above the controls?: 0.3541960389675177\n",
      "\t19: Remove duplicate chars using regex?: 0.3531381989682815\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "question_index = random.randint(0, 99)\n",
    "\n",
    "print(f\"Query question = {questions[question_index]}\")\n",
    "\n",
    "# Get similarity scores for each embedding by using dot-product.\n",
    "scores = np.dot(question_embeddings[question_index], question_embeddings.T)\n",
    "\n",
    "# Print top 20 matches\n",
    "for index, (question, score) in enumerate(\n",
    "    sorted(zip(questions, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "):\n",
    "    print(f\"\\t{index}: {question}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96725ee3-c5bf-443a-877f-001c00f75ead",
   "metadata": {},
   "source": [
    "Save the embeddings in JSONL format. The data must be formatted in JSONL format, which means each embedding dictionary is written as an individual JSON object on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b84d58-1e6e-4658-9ada-ceb998ab4d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings directory: /var/tmp/tmpr745daze\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary file to write embeddings to\n",
    "embeddings_file_path = Path(tempfile.mkdtemp())\n",
    "\n",
    "print(f\"Embeddings directory: {embeddings_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2d17b-36d0-44ed-b983-369e16262cdd",
   "metadata": {},
   "source": [
    "Write embeddings in batches to prevent out-of-memory errors. Notice we are only using 5000 questions so that the embedding creation process and indexing is faster. The dataset contains more than 50,000 questions. This step will take around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59f4a385-b662-4499-86b3-0cec6af6985d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a658c061e3b40749f27cab8614e77d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk of rows from BigQuery:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153d03eb3cac4ecea35886c7145ffaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: 175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed335db433e4073988c23b370166df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: 65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe95a516b04842f2a33cc27b256969a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ca10658f9145db964155df6b767042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: 65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7d272dc91e48f395bb97e1485425c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated: 55\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "BQ_NUM_ROWS = 5000\n",
    "BQ_CHUNK_SIZE = 1000\n",
    "BQ_NUM_CHUNKS = math.ceil(BQ_NUM_ROWS / BQ_CHUNK_SIZE)\n",
    "\n",
    "START_CHUNK = 0\n",
    "\n",
    "# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.\n",
    "API_CALLS_PER_SECOND = 300 / 60\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 5\n",
    "\n",
    "# Loop through each generated dataframe, convert\n",
    "for i, df in tqdm(\n",
    "    enumerate(\n",
    "        query_bigquery_chunks(\n",
    "            max_rows=BQ_NUM_ROWS, rows_per_chunk=BQ_CHUNK_SIZE, start_chunk=START_CHUNK\n",
    "        )\n",
    "    ),\n",
    "    total=BQ_NUM_CHUNKS - START_CHUNK,\n",
    "    position=-1,\n",
    "    desc=\"Chunk of rows from BigQuery\",\n",
    "):\n",
    "    # Create a unique output file for each chunk\n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{i+START_CHUNK}.json\"\n",
    "    )\n",
    "    with open(chunk_path, \"a\") as f:\n",
    "        id_chunk = df.id\n",
    "\n",
    "        # Convert batch to embeddings\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences=df.title_with_body.to_list(),\n",
    "            api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "            batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "        \n",
    "        # Debugging: Check if any embeddings are returned\n",
    "        print(f\"Embeddings generated: {len(question_chunk_embeddings)}\")\n",
    "        if len(question_chunk_embeddings) == 0:\n",
    "            print(\"No embeddings were generated, skipping this chunk.\")\n",
    "            continue\n",
    "\n",
    "        # Check if successful embeddings exist\n",
    "        if not any(is_successful):\n",
    "            print(f\"No successful embeddings in chunk {i}\")\n",
    "            continue\n",
    "\n",
    "        # Append to file\n",
    "        embeddings_formatted = [\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": str(id),\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            for id, embedding in zip(id_chunk[is_successful], question_chunk_embeddings)\n",
    "        ]\n",
    "        f.writelines(embeddings_formatted)\n",
    "\n",
    "        # Delete the DataFrame and any other large data structures\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657019b6-4bfb-4db2-9f10-50903ee96035",
   "metadata": {},
   "source": [
    "## Upload embeddings to Cloud Storage\n",
    "Upload the text-embeddings to Cloud Storage, so that Vertex AI Vector Search can access them later.\n",
    "\n",
    "Define a bucket where you will store your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "635d4a9f-9fa6-407c-8958-5f493b8ae6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d6db8a1-5eab-477f-abec-dfc035a0ab3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-01-b5af813e00ba-unique/...\n"
     ]
    }
   ],
   "source": [
    "# Create your Cloud Storage bucket.\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adafaa34-61d4-455d-bb41-064d5c5b77d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///var/tmp/tmpr745daze/tmpr745daze_0.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmpr745daze/tmpr745daze_1.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmpr745daze/tmpr745daze_2.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmpr745daze/tmpr745daze_3.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmpr745daze/tmpr745daze_4.json [Content-Type=application/json]...\n",
      "- [5/5 files][ 17.5 MiB/ 17.5 MiB] 100% Done                                    \n",
      "Operation completed over 5 objects/17.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Upload the training data to a Google Cloud Storage bucket.\n",
    "remote_folder = f\"{BUCKET_URI}/{embeddings_file_path.stem}/\"\n",
    "! gsutil -m cp -r {embeddings_file_path}/* {remote_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b8956-3949-4fae-ab7a-6d8026cfc8aa",
   "metadata": {},
   "source": [
    "## Create an Index in Vertex AI Vector Search for your embeddings\n",
    "Setup your index name and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38cd977c-1de9-41d5-b9a4-77d8da19c887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"stack_overflow\"\n",
    "DESCRIPTION = \"question titles and bodies from stackoverflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b145a-547f-4af0-930d-a6848a441052",
   "metadata": {},
   "source": [
    "Create the index. Notice that the index reads the embeddings from the Cloud Storage bucket. The indexing process can take from 45 minutes up to 60 minutes. Wait for completion, and then proceed. You can open a different Google Cloud Console page, navigate to Vertex AI Vector search, and see how the index is being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a1984f9-ce89-41d3-9018-13626b58870d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/866155630714/locations/us-west1/indexes/7591600024211947520/operations/7989810700199395328\n",
      "MatchingEngineIndex created. Resource name: projects/866155630714/locations/us-west1/indexes/7591600024211947520\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/866155630714/locations/us-west1/indexes/7591600024211947520')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "DIMENSIONS = 768\n",
    "\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=remote_folder,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=80,\n",
    "    description=DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e617e8e-ce8f-46ed-8745-eaaf42a3bd93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/866155630714/locations/us-west1/indexes/7591600024211947520'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference the index name to make sure it got created successfully.\n",
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b429e2d-516d-42ed-a903-be4e58804856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the resource name, you can retrieve an existing MatchingEngineIndex.\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36831230-e04b-492c-a084-0b79d8a7b61d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664/operations/5859608076453150720\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664')\n"
     ]
    }
   ],
   "source": [
    "# Create an IndexEndpoint so that it can be accessed via an API.\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c8d1f12-ba60-4bd7-84fe-245c116b5097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664/operations/536353316901224448\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[id: \"deployed_index_id_unique\"\n",
       "index: \"projects/866155630714/locations/us-west1/indexes/7591600024211947520\"\n",
       "create_time {\n",
       "  seconds: 1728491016\n",
       "  nanos: 713520000\n",
       "}\n",
       "index_sync_time {\n",
       "  seconds: 1728492596\n",
       "  nanos: 727444000\n",
       "}\n",
       "automatic_resources {\n",
       "  min_replica_count: 2\n",
       "  max_replica_count: 2\n",
       "}\n",
       "deployment_group: \"default\"\n",
       "]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy your index to the created endpoint. This can take up to 15 minutes.\n",
    "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\"\n",
    "\n",
    "DEPLOYED_INDEX_ID\n",
    "\n",
    "\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bbed10-57a4-4584-a10f-ef9cb96b993d",
   "metadata": {},
   "source": [
    "Verify number of declared items matches the number of embeddings. Each IndexEndpoint can have multiple indexes deployed to it. For each index, you can retrieve the number of deployed vectors using the index_endpoint._gca_resource.index_stats.vectors_count. The numbers may not match exactly due to potential rate-limiting failures incurred when using the embedding service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63cbd448-efc7-4800-9fb4-2788a1ebd706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 5000, Actual: 785\n"
     ]
    }
   ],
   "source": [
    "number_of_vectors = sum(\n",
    "    aiplatform.MatchingEngineIndex(\n",
    "        deployed_index.index\n",
    "    )._gca_resource.index_stats.vectors_count\n",
    "    for deployed_index in my_index_endpoint.deployed_indexes\n",
    ")\n",
    "\n",
    "print(f\"Expected: {BQ_NUM_ROWS}, Actual: {number_of_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec794b52-8d61-463f-93f6-20260cb6aa24",
   "metadata": {},
   "source": [
    "## Create online queries\n",
    "After you build your indexes, you may query against the deployed index to find nearest neighbors.\n",
    "\n",
    "Note: For the DOT_PRODUCT_DISTANCE distance type, the \"distance\" property returned with each MatchNeighbor actually refers to the similarity.\n",
    "Create an embedding for a test question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "158d49e0-9e71-4de9-8d69-c1affe0d638b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings = encode_texts_to_embeddings(sentences=[\"Install GPU for Tensorflow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d6ac4b0-9c8c-4cc6-bacd-d87573e0f803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MatchNeighbor(id='8356058', distance=0.46752703189849854, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='69280365', distance=0.46094128489494324, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='28882193', distance=0.45510563254356384, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='69329715', distance=0.45241573452949524, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='9058594', distance=0.4408796727657318, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='43272758', distance=0.43424999713897705, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='46994842', distance=0.4332527816295624, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='56555066', distance=0.4316200911998749, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='26474950', distance=0.4278642535209656, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='63850378', distance=0.4266507029533386, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[])]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the query to retrieve the similar embeddings.\n",
    "NUM_NEIGHBOURS = 10\n",
    "\n",
    "response = my_index_endpoint.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=test_embeddings,\n",
    "    num_neighbors=NUM_NEIGHBOURS,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07eaf09f-c6e0-45c8-8319-36022b974a20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/questions/8356058\n",
      "https://stackoverflow.com/questions/69280365\n",
      "https://stackoverflow.com/questions/28882193\n",
      "https://stackoverflow.com/questions/69329715\n",
      "https://stackoverflow.com/questions/9058594\n",
      "https://stackoverflow.com/questions/43272758\n",
      "https://stackoverflow.com/questions/46994842\n",
      "https://stackoverflow.com/questions/56555066\n",
      "https://stackoverflow.com/questions/26474950\n",
      "https://stackoverflow.com/questions/63850378\n"
     ]
    }
   ],
   "source": [
    "# Verify that the retrieved results are relevant by checking the StackOverflow links.\n",
    "\n",
    "for match_index, neighbor in enumerate(response[0]):\n",
    "    print(f\"https://stackoverflow.com/questions/{neighbor.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75a823-b43e-4358-875f-4e3f807cdb0a",
   "metadata": {},
   "source": [
    "# Clean up the Google Cloud environment\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the lab. You can also manually delete resources that you created by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "667a3bfc-b5cc-4bbc-866b-6b5ff59560c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying MatchingEngineIndexEndpoint index_endpoint: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "Undeploy MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664/operations/394489928639053824\n",
      "MatchingEngineIndexEndpoint index_endpoint undeployed. Resource name: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "Deleting MatchingEngineIndexEndpoint : projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "MatchingEngineIndexEndpoint deleted. . Resource name: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "Deleting MatchingEngineIndexEndpoint resource: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664\n",
      "Delete MatchingEngineIndexEndpoint backing LRO: projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664/operations/7861458110819336192\n",
      "MatchingEngineIndexEndpoint resource projects/866155630714/locations/us-west1/indexEndpoints/6677404484227825664 deleted.\n",
      "Deleting MatchingEngineIndex : projects/866155630714/locations/us-west1/indexes/7591600024211947520\n",
      "MatchingEngineIndex deleted. . Resource name: projects/866155630714/locations/us-west1/indexes/7591600024211947520\n",
      "Deleting MatchingEngineIndex resource: projects/866155630714/locations/us-west1/indexes/7591600024211947520\n",
      "Delete MatchingEngineIndex backing LRO: projects/866155630714/locations/us-west1/indexes/7591600024211947520/operations/3910675337708568576\n",
      "MatchingEngineIndex resource projects/866155630714/locations/us-west1/indexes/7591600024211947520 deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "delete_bucket = False\n",
    "\n",
    "# Force undeployment of indexes and delete endpoint\n",
    "my_index_endpoint.delete(force=True)\n",
    "\n",
    "# Delete indexes\n",
    "tree_ah_index.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4bc7b-cbda-4caa-b1e0-414545e1fad5",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "You have now completed the lab! In this lab, you created a semantic search system using Vertex AI Text-Embeddings API and Vertex AI Vector Search. You did that with a big dataset and learned how to work around API limits and quotas, the same way you would have to in a production environment.\n",
    "\n",
    "### Next steps\n",
    "Check out the [Generative AI on Vertex AI documentation.](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)\n",
    "Learn more about Generative AI on the [Google Cloud Tech YouTube channel.](https://www.youtube.com/@googlecloudtech/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7f9e9-c718-4c7c-858c-974e9d3c1527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
