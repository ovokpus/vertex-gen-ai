{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602fc0a3-c951-4d7b-8d03-d81d407b1c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.22.4\n",
      "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, pandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "contourpy 1.3.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "dataproc-jupyter-plugin 0.1.80 requires aiohttp~=3.9.5, but you have aiohttp 3.10.5 which is incompatible.\n",
      "pywavelets 1.7.0 requires numpy<3,>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.22.4 pandas-1.5.3\n",
      "Collecting pgvector==0.1.8\n",
      "  Downloading pgvector-0.1.8-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pgvector==0.1.8) (1.22.4)\n",
      "Downloading pgvector-0.1.8-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: pgvector\n",
      "Successfully installed pgvector-0.1.8\n",
      "Collecting langchain==0.0.196\n",
      "  Downloading langchain-0.0.196-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers==4.30.1\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl.metadata (113 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (3.10.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (4.0.3)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.196)\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting langchainplus-sdk>=0.0.7 (from langchain==0.0.196)\n",
      "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.196)\n",
      "  Downloading numexpr-2.10.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (1.22.4)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.196)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (1.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.196) (2.32.3)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.196)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.1) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.1)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.1) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.1) (2024.9.11)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.1)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.30.1)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.1) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.196) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.196) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.196) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.196) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.196) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.196) (1.11.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.196)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.196)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.1) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.1) (4.12.2)\n",
      "Collecting numpy<2,>=1 (from langchain==0.0.196)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.196) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.196) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.196) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.196) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.196) (3.1.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.196)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.0.196-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
      "Downloading numexpr-2.10.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (405 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tokenizers, tenacity, safetensors, numpy, mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, numexpr, langchainplus-sdk, huggingface-hub, transformers, dataclasses-json, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 huggingface-hub-0.25.2 langchain-0.0.196 langchainplus-sdk-0.0.20 marshmallow-3.22.0 mypy-extensions-1.0.0 numexpr-2.10.1 numpy-1.26.4 openapi-schema-pydantic-1.2.4 safetensors-0.4.5 tenacity-8.5.0 tokenizers-0.13.3 transformers-4.30.1 typing-inspect-0.9.0\n",
      "Collecting google-cloud-aiplatform==1.26.0\n",
      "  Downloading google_cloud_aiplatform-1.26.0-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (2.21.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.26.0) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.26.0) (4.25.5)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.26.0) (24.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.26.0) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.26.0) (3.26.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.26.0) (1.12.3)\n",
      "Collecting shapely<2.0.0 (from google-cloud-aiplatform==1.26.0)\n",
      "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (1.63.1)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (2.35.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (1.48.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (1.48.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.0) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.26.0) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.26.0) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (1.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (4.9)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (2024.8.30)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.26.0) (0.6.1)\n",
      "Downloading google_cloud_aiplatform-1.26.0-py2.py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely, google-cloud-aiplatform\n",
      "  Attempting uninstall: shapely\n",
      "    Found existing installation: shapely 2.0.6\n",
      "    Uninstalling shapely-2.0.6:\n",
      "      Successfully uninstalled shapely-2.0.6\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.70.0\n",
      "    Uninstalling google-cloud-aiplatform-1.70.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.70.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "geopandas 1.0.1 requires shapely>=2.0.0, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.26.0 shapely-1.8.5.post1\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "Collecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.7.0 requires protobuf<5,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.3.0 requires protobuf<5,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "dataproc-jupyter-plugin 0.1.80 requires aiohttp~=3.9.5, but you have aiohttp 3.10.5 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.21.0 which is incompatible.\n",
      "google-cloud-pubsub 2.21.4 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.48.0 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.19.6\n",
      "Collecting shapely==2.0.6\n",
      "  Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely==2.0.6) (1.26.4)\n",
      "Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely\n",
      "  Attempting uninstall: shapely\n",
      "    Found existing installation: Shapely 1.8.5.post1\n",
      "    Uninstalling Shapely-1.8.5.post1:\n",
      "      Successfully uninstalled Shapely-1.8.5.post1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-aiplatform 1.26.0 requires shapely<2.0.0, but you have shapely 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed shapely-2.0.6\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy==1.22.4 pandas==1.5.3\n",
    "! pip install pgvector==0.1.8\n",
    "! pip install langchain==0.0.196 transformers==4.30.1\n",
    "! pip install google-cloud-aiplatform==1.26.0\n",
    "! pip install psycopg2-binary\n",
    "! pip install protobuf==3.20.3\n",
    "! pip install shapely==2.0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f6ae47-5a88-4b76-be7f-a30e4e0efa13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ea0e8-6b35-4604-9ec5-e1599ea12cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215b3dbf-ff03-458c-9515-db9100b6f710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n",
      "('74a695e3675efc2aad11ed73c46db29b', 'Slip N Slide Triple Racer with Slide Boogies', 'Triple Racer Slip and Slide with Boogie Boards. The unit is 16 foot long. The unit has 3 sliding lanes.', 37.21)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Replace with your AlloyDB cluster credentials\n",
    "cluster_ip_address = \"10.111.0.2\"\n",
    "database_user = \"postgres\"\n",
    "database_password = \"postgres\"\n",
    "\n",
    "# Set environment variables for psql connection\n",
    "os.environ[\"PGHOST\"] = cluster_ip_address\n",
    "os.environ[\"PGUSER\"] = database_user\n",
    "os.environ[\"PGPASSWORD\"] = database_password\n",
    "\n",
    "# Establish a connection to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=cluster_ip_address,\n",
    "        user=database_user,\n",
    "        password=database_password\n",
    "    )\n",
    "    print(\"Connected to the database successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection error:\", e)\n",
    "    exit(1)\n",
    "\n",
    "# Read the dataset from the URL\n",
    "DATASET_URL = \"https://github.com/GoogleCloudPlatform/python-docs-samples/raw/main/cloud-sql/postgres/pgvector/data/retail_toy_dataset.csv\"\n",
    "df = pd.read_csv(DATASET_URL)\n",
    "\n",
    "# Select desired columns and drop missing values\n",
    "df = df.loc[:, [\"product_id\", \"product_name\", \"description\", \"list_price\"]]\n",
    "df = df.dropna()\n",
    "\n",
    "# Save the DataFrame to the AlloyDB cluster\n",
    "df.to_sql('products', con=f'postgresql://{cluster_ip_address}', if_exists='replace', index=False)\n",
    "\n",
    "# Retrieve data from the 'products' table\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM products\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(results[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5080ef05-0839-4339-bcb7-3d70f905b2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\".\", \"\\n\"],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    ")\n",
    "max_documents = 60\n",
    "chunked = []\n",
    "for index, row in df.iterrows():\n",
    "    product_id = row[\"product_id\"]\n",
    "    desc = row[\"description\"]\n",
    "    splits = text_splitter.create_documents([desc])\n",
    "    if len(chunked) < max_documents:\n",
    "        for s in splits:\n",
    "            r = {\"product_id\": product_id, \"content\": s.page_content}\n",
    "            chunked.append(r)\n",
    "    else:\n",
    "        break\n",
    "print(len(chunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfacba88-afa6-4670-a24a-6a523560cca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7e8697b5b7cdb5a40daf54caf1435cd5</td>\n",
       "      <td>Rock, paper, scissors is a great way to resolv...</td>\n",
       "      <td>[0.040036752820014954, -0.025681596249341965, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7e8697b5b7cdb5a40daf54caf1435cd5</td>\n",
       "      <td>games, creating your own game, and friends and...</td>\n",
       "      <td>[0.01725427247583866, -0.002125497441738844, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7de8b315b3cb91f3680eb5b88a20dcee</td>\n",
       "      <td>Turn any small bicycle into an instrument for ...</td>\n",
       "      <td>[-0.004991121590137482, -0.06028855964541435, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7de8b315b3cb91f3680eb5b88a20dcee</td>\n",
       "      <td>teel brackets stand up to heavy use. Customiza...</td>\n",
       "      <td>[0.00582809979096055, -0.056259434670209885, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7de8b315b3cb91f3680eb5b88a20dcee</td>\n",
       "      <td>es.com. Follow ride Schwinn on: Twitter. Faceb...</td>\n",
       "      <td>[0.0065319109708070755, -0.05318767577409744, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         product_id  \\\n",
       "0  7e8697b5b7cdb5a40daf54caf1435cd5   \n",
       "1  7e8697b5b7cdb5a40daf54caf1435cd5   \n",
       "2  7de8b315b3cb91f3680eb5b88a20dcee   \n",
       "3  7de8b315b3cb91f3680eb5b88a20dcee   \n",
       "4  7de8b315b3cb91f3680eb5b88a20dcee   \n",
       "\n",
       "                                             content  \\\n",
       "0  Rock, paper, scissors is a great way to resolv...   \n",
       "1  games, creating your own game, and friends and...   \n",
       "2  Turn any small bicycle into an instrument for ...   \n",
       "3  teel brackets stand up to heavy use. Customiza...   \n",
       "4  es.com. Follow ride Schwinn on: Twitter. Faceb...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.040036752820014954, -0.025681596249341965, ...  \n",
       "1  [0.01725427247583866, -0.002125497441738844, -...  \n",
       "2  [-0.004991121590137482, -0.06028855964541435, ...  \n",
       "3  [0.00582809979096055, -0.056259434670209885, 0...  \n",
       "4  [0.0065319109708070755, -0.05318767577409744, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the vector embeddings for each chunk of text.\n",
    "# This code snippet may run for a few minutes.\n",
    "\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from google.cloud import aiplatform\n",
    "import time\n",
    "\n",
    "aiplatform.init(project=f\"qwiklabs-gcp-02-be06f8b1cebc\", location=f\"us-central1\")\n",
    "embeddings_service = VertexAIEmbeddings()\n",
    "\n",
    "# Helper function to retry failed API requests with exponential backoff.\n",
    "def retry_with_backoff(func, *args, retry_delay=5, backoff_factor=2, **kwargs):\n",
    "    max_attempts = 10\n",
    "    retries = 0\n",
    "    for i in range(max_attempts):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "            retries += 1\n",
    "            wait = retry_delay * (backoff_factor**retries)\n",
    "            print(f\"Retry after waiting for {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "batch_size = 5\n",
    "for i in range(0, len(chunked), batch_size):\n",
    "    request = [x[\"content\"] for x in chunked[i : i + batch_size]]\n",
    "    response = retry_with_backoff(embeddings_service.embed_documents, request)\n",
    "    # Store the retrieved vector embeddings for each chunk back.\n",
    "    for x, e in zip(chunked[i : i + batch_size], response):\n",
    "        x[\"embedding\"] = e\n",
    "\n",
    "# Store the generated embeddings in a pandas dataframe.\n",
    "product_embeddings = pd.DataFrame(chunked)\n",
    "product_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d057afe4-291a-40dc-a8f9-74acc1ba37dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the 'product_embeddings' table successfully\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# AlloyDB cluster connection details (replace with your actual values)\n",
    "cluster_ip_address = \"10.111.0.2\"\n",
    "database_user = \"postgres\"\n",
    "database_password = \"postgres\"\n",
    "\n",
    "# Connect to AlloyDB cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=cluster_ip_address,\n",
    "    user=database_user,\n",
    "    password=database_password\n",
    ")\n",
    "\n",
    "# Create cursor for executing SQL commands\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Ensure vector extension is installed\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "# Drop existing table (if it exists)\n",
    "cur.execute(\"DROP TABLE IF EXISTS product_embeddings\")\n",
    "\n",
    "# Create the `product_embeddings` table\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE product_embeddings(\n",
    "        product_id VARCHAR(1024) NOT NULL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        embedding vector(768)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Register the vector type\n",
    "register_vector(conn)\n",
    "\n",
    "# Store vector embeddings in the table\n",
    "for index, row in product_embeddings.iterrows():\n",
    "    cur.execute(\"SELECT EXISTS(SELECT 1 FROM product_embeddings WHERE product_id = %s)\", (row[\"product_id\"],))\n",
    "    if not cur.fetchone()[0]:  # Product ID doesn't exist, insert\n",
    "        cur.execute(\"INSERT INTO product_embeddings (product_id, content, embedding) VALUES (%s, %s, %s)\", (row[\"product_id\"], row[\"content\"], row[\"embedding\"]))\n",
    "    else:  # Product ID exists, update\n",
    "        cur.execute(\"UPDATE product_embeddings SET content = %s, embedding = %s WHERE product_id = %s\", (row[\"content\"], row[\"embedding\"], row[\"product_id\"]))\n",
    "\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"Created the 'product_embeddings' table successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b26961-0a69-4dbc-b932-f67190ec253c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an HNSW Index successfully\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# AlloyDB cluster connection details (replace with your actual values)\n",
    "cluster_ip_address = \"10.111.0.2\"\n",
    "database_user = \"postgres\"\n",
    "database_password = \"postgres\"\n",
    "\n",
    "m = 24\n",
    "ef_construction = 100\n",
    "operator = \"vector_cosine_ops\"\n",
    "\n",
    "# Connect to AlloyDB cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=cluster_ip_address,\n",
    "    user=database_user,\n",
    "    password=database_password\n",
    ")\n",
    "\n",
    "# Register the vector type\n",
    "register_vector(conn)\n",
    "\n",
    "# Create the HNSW index on the `product_embeddings` table\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    f\"\"\"CREATE INDEX ON product_embeddings\n",
    "        USING hnsw(embedding {operator})\n",
    "        WITH (m = {m}, ef_construction = {ef_construction})\n",
    "    \"\"\"\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"Created an HNSW Index successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c702bc75-04f5-4f3f-84d0-49f7d945d02c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an IVFFLAT Index successfully\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# AlloyDB cluster connection details (replace with your actual values)\n",
    "cluster_ip_address = \"10.111.0.2\"\n",
    "database_user = \"postgres\"\n",
    "database_password = \"postgres\"\n",
    "\n",
    "lists = 100\n",
    "operator = \"vector_cosine_ops\"\n",
    "\n",
    "# Connect to AlloyDB cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=cluster_ip_address,\n",
    "    user=database_user,\n",
    "    password=database_password\n",
    ")\n",
    "\n",
    "# Register the vector type\n",
    "register_vector(conn)\n",
    "\n",
    "# Create the IVFFLAT index on the `product_embeddings` table\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    f\"\"\"CREATE INDEX ON product_embeddings\n",
    "        USING ivfflat(embedding {operator})\n",
    "        WITH (lists = {lists})\n",
    "    \"\"\"\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"Created an IVFFLAT Index successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6faed1-478d-4428-95e9-da9013760847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        product_name  list_price  \\\n",
      "0                    12\"-20\" Schwinn Training Wheels       28.17   \n",
      "1       Slip N Slide Triple Racer with Slide Boogies       37.21   \n",
      "2  Polaris 39-310 5-Liter Zippered Super Bag for ...       39.47   \n",
      "3  Sandbox Castle 2-in-1 Sand and Water Table wit...       60.49   \n",
      "4  Jensen S100T Commercial Tot Full Bucket Rubber...       90.18   \n",
      "\n",
      "                                         description  \n",
      "0  Turn any small bicycle into an instrument for ...  \n",
      "1  Triple Racer Slip and Slide with Boogie Boards...  \n",
      "2  Keep your pool water sparkling clean all seaso...  \n",
      "3  Package Includes Sandbox Castle 2-in-1 Sand an...  \n",
      "4  This is a fully enclosed one piece infant seat...  \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import pandas as pd\n",
    "\n",
    "# AlloyDB cluster connection details (replace with your actual values)\n",
    "cluster_ip_address = \"10.111.0.2\"\n",
    "database_user = \"postgres\"\n",
    "database_password = \"postgres\"\n",
    "\n",
    "toy = \"playing card games\"\n",
    "min_price = 25\n",
    "max_price = 100\n",
    "\n",
    "# Connect to AlloyDB cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=cluster_ip_address,\n",
    "    user=database_user,\n",
    "    password=database_password\n",
    ")\n",
    "\n",
    "# Register the vector type\n",
    "register_vector(conn)\n",
    "\n",
    "# Get the query embedding\n",
    "qe = embeddings_service.embed_query([toy])\n",
    "\n",
    "# Perform the similarity search and filtering\n",
    "cur = conn.cursor()\n",
    "similarity_threshold = 0.1\n",
    "num_matches = 50\n",
    "# Pass 'qe' twice to match the number of placeholders in the query\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    WITH vector_matches AS (\n",
    "        SELECT product_id, 1 - (embedding <=> %s::vector) AS similarity\n",
    "        FROM product_embeddings\n",
    "        WHERE 1 - (embedding <=> %s::vector) > %s\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT %s\n",
    "    )\n",
    "    SELECT product_name, list_price, description\n",
    "    FROM products\n",
    "    WHERE product_id IN (SELECT product_id FROM vector_matches)\n",
    "    AND list_price >= %s AND list_price <= %s\n",
    "    \"\"\",\n",
    "    (qe, qe, similarity_threshold, num_matches, min_price, max_price)\n",
    ")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Process the results\n",
    "matches = []\n",
    "for r in results:\n",
    "    try:\n",
    "        list_price = round(float(r[2]), 2)  # Attempt conversion and rounding\n",
    "    except ValueError:\n",
    "        list_price = r[2] \n",
    "    matches.append({\n",
    "        \"product_name\": r[0],\n",
    "        \"list_price\": r[1],\n",
    "        \"description\": r[2]\n",
    "    })\n",
    "\n",
    "# Display the results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "print(matches_df.head(5))\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84281713-6b96-4df5-8be3-44214334e46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please fill in these values.\n",
    "user_query = \"Do you have a toy set that teaches numbers and letters to kids?\"  # @param {type:\"string\"}\n",
    "min_price = 20  # @param {type:\"integer\"}\n",
    "max_price = 100  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b992a9ef-9de6-4dde-933d-86fe6b56fb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qe = embeddings_service.embed_query([user_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8359ca2-cbff-40d2-a667-2d8c0b5e7b17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        product_name  list_price  \\\n",
      "0                    12\"-20\" Schwinn Training Wheels       28.17   \n",
      "1       Slip N Slide Triple Racer with Slide Boogies       37.21   \n",
      "2  Polaris 39-310 5-Liter Zippered Super Bag for ...       39.47   \n",
      "3  Sandbox Castle 2-in-1 Sand and Water Table wit...       60.49   \n",
      "4  Jensen S100T Commercial Tot Full Bucket Rubber...       90.18   \n",
      "\n",
      "                                         description  \n",
      "0  Turn any small bicycle into an instrument for ...  \n",
      "1  Triple Racer Slip and Slide with Boogie Boards...  \n",
      "2  Keep your pool water sparkling clean all seaso...  \n",
      "3  Package Includes Sandbox Castle 2-in-1 Sand an...  \n",
      "4  This is a fully enclosed one piece infant seat...  \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def main(user_query,min_price,max_price):\n",
    "    try:\n",
    "        # AlloyDB cluster connection details (replace with your actual values)\n",
    "        cluster_ip_address = \"10.111.0.2\"\n",
    "        database_user = \"postgres\"\n",
    "        database_password = \"postgres\"\n",
    "        \n",
    "        # Connect to AlloyDB cluster\n",
    "        conn = psycopg2.connect(\n",
    "        host=cluster_ip_address,\n",
    "        user=database_user,\n",
    "        password=database_password)\n",
    "\n",
    "        # Register the vector type\n",
    "        register_vector(conn)\n",
    "\n",
    "        # Get the query embedding\n",
    "        qe = embeddings_service.embed_query([user_query])\n",
    "        \n",
    "        # Perform the similarity search and filtering\n",
    "        cur = conn.cursor()\n",
    "        similarity_threshold = 0.1\n",
    "        num_matches = 50\n",
    "        \n",
    "        # Pass 'qe' twice to match the number of placeholders in the query\n",
    "        cur.execute(\n",
    "                    \"\"\"\n",
    "                        WITH vector_matches AS (\n",
    "                        SELECT product_id, 1 - (embedding <=> %s::vector) AS similarity\n",
    "                        FROM product_embeddings\n",
    "                        WHERE 1 - (embedding <=> %s::vector) > %s\n",
    "                        ORDER BY similarity DESC\n",
    "                        LIMIT %s\n",
    "                )\n",
    "                SELECT product_name, list_price, description\n",
    "                FROM products\n",
    "                WHERE product_id IN (SELECT product_id FROM vector_matches)\n",
    "                AND list_price >= %s AND list_price <= %s\n",
    "                \"\"\",\n",
    "                (qe, qe, similarity_threshold, num_matches, min_price, max_price))\n",
    "        results = cur.fetchall()\n",
    "        # Process the results\n",
    "        matches = []\n",
    "        for r in results:\n",
    "            try:\n",
    "                list_price = round(float(r[2]), 2)  # Attempt conversion and rounding\n",
    "            except ValueError:\n",
    "                    list_price = r[2] \n",
    "                    matches.append({\n",
    "                        \"product_name\": r[0],\n",
    "                        \"list_price\": r[1],\n",
    "                        \"description\": r[2]\n",
    "                    })\n",
    "                    # Display the results\n",
    "        matches_df = pd.DataFrame(matches)\n",
    "        print(matches_df.head(5))\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during database operations: {e}\")\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "    return\n",
    "\n",
    "# Call the main function (no need for asyncio in this context)\n",
    "main(\"Do you have a toy set that teaches numbers and letters to kids?\",25,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a7c504-9342-4a8b-945a-5ab0579376f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e260dbe166714ae9a644bdee80c82e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53daa4e6e75b4a5bae15389bf9d4d6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080051437fe54a758907931f240390c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a74b14e21d4f45a9983b3abd015d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c587de8af8341179df3444b8e29f6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " 1. **Toy Name**: Sandbox Castle 2-in-1 Sand and Water Table with Beach Playset\n",
       "\n",
       "2. **Price**: $60.49\n",
       "\n",
       "3. **Features**:\n",
       "\n",
       "- Includes 14-piece sand tools such as watering can, shovel, rake, castle and shell sand molds, sailboat, and bridges.\n",
       "- Great activity for indoor or outdoor play and small enough to take on trips.\n",
       "- Colorful, easy to assemble sand water play table with dual sink design."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using LangChain for summarization and efficient context building.\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "llm = VertexAI()\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "              You will be given a detailed description of a toy product.\n",
    "              This description is enclosed in triple backticks (```).\n",
    "              Using this description only, extract the name of the toy,\n",
    "              the price of the toy and its features.\n",
    "\n",
    "              ```{text}```\n",
    "              SUMMARY:\n",
    "              \"\"\"\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                You will be given a detailed description different toy products\n",
    "                enclosed in triple backticks (```) and a question enclosed in\n",
    "                double backticks(``).\n",
    "                Select one toy that is most relevant to answer the question.\n",
    "                Using that selected toy description, answer the following\n",
    "                question in as much detail as possible.\n",
    "                You should only use the information in the description.\n",
    "                Your answer should include the name of the toy, the price of the toy\n",
    "                and its features. Your answer should be less than 200 words.\n",
    "                Your answer should be in Markdown in a numbered list format.\n",
    "\n",
    "\n",
    "                Description:\n",
    "                ```{text}```\n",
    "\n",
    "\n",
    "                Question:\n",
    "                ``{user_query}``\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\", \"user_query\"]\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=str(t)) for t in matches]\n",
    "chain = load_summarize_chain(\n",
    "    llm, chain_type=\"map_reduce\", map_prompt=map_prompt, combine_prompt=combine_prompt\n",
    ")\n",
    "answer = chain.run(\n",
    "    {\n",
    "        \"input_documents\": docs,\n",
    "        \"user_query\": user_query,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f9a779-b461-419c-87c0-9bdb75b9f738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please fill in these values.\n",
    "creative_prompt = \"A bicycle with brand name 'Roadstar bike' for kids that comes with training wheels and helmet.\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1577724-7ec5-44ba-9721-de58eae63b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turn any small bicycle into an instrument for learning to ride with the Schwinn 12\"-20\" Training Wheels. They feature a slotted design to fit 12\" to 20\" bikes. The training wheels are easy to assemble, install and remove, so that when your little one is able to ride without assistance, you can take them off. These bicycle training wheels include steel brackets and rubber tires that can stand up to heavy use. Training Wheels, Fits 12 inches - 20 inches bicycles. Est. 1895. Durable Construction: Steel brackets stand up to heavy use. Customizable: Two sets of wheel decals included. Features: Fits Most Childrens Bicycles: Intended for 12 inch - 20 inch bicycles. Steel Brackets: Offer increased durability. Includes two sets of wheel decals: Learn how to ride in style - see images below. Easy to Adjust: Slotted design for size adjustment. Includes: One pair of training wheels, four decals, installation instructions, and all mounting hardware. Tools required: Adjustable wrench. www.schwinnbikes.com. Follow ride Schwinn on: Twitter. Facebook. Made in China. Training Wheels,Fits 12 inches - 20 inches bicycles. Est. 1895. Durable Construction: Steel brackets stand up to heavy use. Customizable: Two sets of wheel decals included. Features: Fits Most Childrens Bicycles: Intended for 12 inch - 20 inch bicycles. Steel Brackets: Offer increased durability. Includes two sets of wheel decals: Learn how to ride in style - see images below. Easy to Adjust: Slotted design for size adjustment. Includes: One pair of training wheels, four decals, installation instructions, and all mounting hardware. Tools required: Adjustable wrench. www.schwinnbikes.com. Follow ride Schwinn on: Twitter. Facebook. Made in China.']\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # AlloyDB cluster connection details (replace with your actual values)\n",
    "        cluster_ip_address = \"10.111.0.2\"\n",
    "        database_user = \"postgres\"\n",
    "        database_password = \"postgres\"\n",
    "        \n",
    "        # Connect to AlloyDB cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=cluster_ip_address,\n",
    "            user=database_user,\n",
    "            password=database_password)\n",
    "\n",
    "        # Register the vector type\n",
    "        register_vector(conn)\n",
    "\n",
    "        # Get the query embedding\n",
    "        qe = embeddings_service.embed_query([creative_prompt])\n",
    "        qe_str = \"[%s]\" % (\",\".join([str(x) for x in qe]))\n",
    "        matches = []\n",
    "        similarity_threshold = 0.7\n",
    "\n",
    "        # Perform the similarity search and filtering\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Pass 'qe' twice to match the number of placeholders in the query\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            WITH vector_matches AS (\n",
    "                SELECT product_id, 1 - (embedding <=> %s::vector) AS similarity\n",
    "                FROM product_embeddings\n",
    "                WHERE 1 - (embedding <=> %s::vector) > %s\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT 1\n",
    "            )\n",
    "            SELECT description FROM products\n",
    "            WHERE product_id IN (SELECT product_id FROM vector_matches)\n",
    "            \"\"\",\n",
    "            (qe, qe, similarity_threshold))\n",
    "        results = cur.fetchall()\n",
    "        # Process the results\n",
    "        for r in results:\n",
    "            matches.append(r[0])\n",
    "\n",
    "        print(matches)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during database operations: {e}\")\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "    return\n",
    "\n",
    "# Call the main function (no need for asyncio in this context)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97f02826-3881-4d47-8d1d-89a60442f9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Roadstar Bike for Kids with Training Wheels and Helmet**\n",
       "\n",
       "         * **Durable Construction:** Sturdy steel frame with a scratch-resistant finish.\n",
       "         * **Adjustable Seat and Handlebars:** Easily adjust the seat and handlebars to fit your child's height.\n",
       "         * **12-20 Inch Training Wheels:** The included training wheels provide stability for young riders, and they're removable when your child is ready to ride without them.\n",
       "         * **Quick-Release Seat Post:** Easily adjust the seat height without tools.\n",
       "         * **Front and Rear Handbrakes:** Provide sure stopping power.\n",
       "         "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "template = \"\"\"\n",
    "            You are given descriptions about some similar kind of toys in the context.\n",
    "            This context is enclosed in triple backticks (```).\n",
    "            Combine these descriptions and adapt them to match the specifications in\n",
    "            the initial prompt. All the information from the initial prompt must\n",
    "            be included. You are allowed to be as creative as possible,\n",
    "            and describe the new toy in as much detail. Your answer should be\n",
    "            in markdown in lists and less than 200 words.\n",
    "\n",
    "            Context:\n",
    "            ```{context}```\n",
    "\n",
    "            Initial Prompt:\n",
    "            {creative_prompt}\n",
    "\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"creative_prompt\"]\n",
    ")\n",
    "\n",
    "# Increase the `temperature` to allow more creative writing freedom.\n",
    "llm = VertexAI(temperature=0.7)\n",
    "\n",
    "# Assuming each dictionary in `matches` has a `description` key:\n",
    "context = \"\\n\".join(\n",
    "    match[\"description\"] for match in matches if isinstance(match, dict)\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "answer = llm_chain.run(\n",
    "    {\n",
    "        \"context\": context,\n",
    "        \"creative_prompt\": creative_prompt,\n",
    "   }\n",
    ")\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b67fe-4728-498c-803a-171c545c092d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
